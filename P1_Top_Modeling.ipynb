{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJET 1 TOPIC MODELING AVIS DES PRODUITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1. Prétraitement des Avis de Produits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import defaultdict\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'No white background! It’s clear!', 'text': 'I bought this bc I thought it had the nice white background. Turns out it’s clear & since my phone is blue it doesn’t look anything like this.  If I had known that I would have purchased something else. It works ok.'}, {'title': 'Awesome!  Great price!  Works well!', 'text': 'Perfect. How pissed am I that I recently paid $20 for 1 Fitbit cable and promptly lost the damned thing?  Extremely pissed!  I keep the spare in my medicine bag so hopefully I won’t lose it and my grandson can’t get to it and try to use it as a belt or a dog leash or any of the other nutty things he’s been using the other one for.'}, {'title': 'Worked but took an hour to install', 'text': 'Overall very happy with the end result. If you hate puzzles don’t do it. I love puzzles and it worked for me. Took a lot of concentration and attention to detail and about an hour! The YouTube video helped a ton with installing the new screen. Highly recommend using a how to video when replacing your screen. The tools and supplies they provided were adequate. I did use additional tools from my home to successfully installed a new screen. My screws on the inside of the iPhone were stuck and I had to use an X-Acto knife to get them to come out. The glass Screen for the iPhone was beautiful and worked great. The screen protector that was additional came cracked (Not a big deal as it was extra in my eyes). I did need to use the X-Acto knife to cut off part of a plastic piece to make the final fit. So yes I modified the screen and instructions but ended up working great for me.<br /><br />I was very careful with all of the circuit boards and connections as recommended on the YouTube video. My screen replacement was very successful and I’m very happy with how it turned out.'}, {'title': 'Decent', 'text': 'Lasted about 9 months then the lock button broke off. Decent product but costing scrapes off like crazy.  I shredded this case. Protected my phone tho'}, {'title': 'LOVE IT!', 'text': 'LOVE THIS CASE! Works better than my expensive $35 cases! lol'}]\n"
     ]
    }
   ],
   "source": [
    "# fichier JSONL\n",
    "reviews_file = 'reviews.jsonl'\n",
    "\n",
    "# Charger les données\n",
    "with open(reviews_file, 'r') as file:\n",
    "    reviews = [json.loads(line) for line in file]\n",
    "\n",
    "# Selection des champs pertinents (title et text) et création d'une liste de documents\n",
    "documents = [{'title': review['title'], 'text': review['text']} for review in reviews]\n",
    "\n",
    "# Les 5 premiers documents\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Traitement linguistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'No white background! It’s clear!', 'text': 'I bought this bc I thought it had the nice white background. Turns out it’s clear & since my phone is blue it doesn’t look anything like this.  If I had known that I would have purchased something else. It works ok.', 'tokens': ['I', 'bought', 'this', 'bc', 'I', 'thought', 'it', 'had', 'the', 'nice', 'white', 'background', '.', 'Turns', 'out', 'it', '’', 's', 'clear', '&', 'since', 'my', 'phone', 'is', 'blue', 'it', 'doesn', '’', 't', 'look', 'anything', 'like', 'this', '.', 'If', 'I', 'had', 'known', 'that', 'I', 'would', 'have', 'purchased', 'something', 'else', '.', 'It', 'works', 'ok', '.']}, {'title': 'Awesome!  Great price!  Works well!', 'text': 'Perfect. How pissed am I that I recently paid $20 for 1 Fitbit cable and promptly lost the damned thing?  Extremely pissed!  I keep the spare in my medicine bag so hopefully I won’t lose it and my grandson can’t get to it and try to use it as a belt or a dog leash or any of the other nutty things he’s been using the other one for.', 'tokens': ['Perfect', '.', 'How', 'pissed', 'am', 'I', 'that', 'I', 'recently', 'paid', '$', '20', 'for', '1', 'Fitbit', 'cable', 'and', 'promptly', 'lost', 'the', 'damned', 'thing', '?', 'Extremely', 'pissed', '!', 'I', 'keep', 'the', 'spare', 'in', 'my', 'medicine', 'bag', 'so', 'hopefully', 'I', 'won', '’', 't', 'lose', 'it', 'and', 'my', 'grandson', 'can', '’', 't', 'get', 'to', 'it', 'and', 'try', 'to', 'use', 'it', 'as', 'a', 'belt', 'or', 'a', 'dog', 'leash', 'or', 'any', 'of', 'the', 'other', 'nutty', 'things', 'he', '’', 's', 'been', 'using', 'the', 'other', 'one', 'for', '.']}, {'title': 'Worked but took an hour to install', 'text': 'Overall very happy with the end result. If you hate puzzles don’t do it. I love puzzles and it worked for me. Took a lot of concentration and attention to detail and about an hour! The YouTube video helped a ton with installing the new screen. Highly recommend using a how to video when replacing your screen. The tools and supplies they provided were adequate. I did use additional tools from my home to successfully installed a new screen. My screws on the inside of the iPhone were stuck and I had to use an X-Acto knife to get them to come out. The glass Screen for the iPhone was beautiful and worked great. The screen protector that was additional came cracked (Not a big deal as it was extra in my eyes). I did need to use the X-Acto knife to cut off part of a plastic piece to make the final fit. So yes I modified the screen and instructions but ended up working great for me.<br /><br />I was very careful with all of the circuit boards and connections as recommended on the YouTube video. My screen replacement was very successful and I’m very happy with how it turned out.', 'tokens': ['Overall', 'very', 'happy', 'with', 'the', 'end', 'result', '.', 'If', 'you', 'hate', 'puzzles', 'don', '’', 't', 'do', 'it', '.', 'I', 'love', 'puzzles', 'and', 'it', 'worked', 'for', 'me', '.', 'Took', 'a', 'lot', 'of', 'concentration', 'and', 'attention', 'to', 'detail', 'and', 'about', 'an', 'hour', '!', 'The', 'YouTube', 'video', 'helped', 'a', 'ton', 'with', 'installing', 'the', 'new', 'screen', '.', 'Highly', 'recommend', 'using', 'a', 'how', 'to', 'video', 'when', 'replacing', 'your', 'screen', '.', 'The', 'tools', 'and', 'supplies', 'they', 'provided', 'were', 'adequate', '.', 'I', 'did', 'use', 'additional', 'tools', 'from', 'my', 'home', 'to', 'successfully', 'installed', 'a', 'new', 'screen', '.', 'My', 'screws', 'on', 'the', 'inside', 'of', 'the', 'iPhone', 'were', 'stuck', 'and', 'I', 'had', 'to', 'use', 'an', 'X-Acto', 'knife', 'to', 'get', 'them', 'to', 'come', 'out', '.', 'The', 'glass', 'Screen', 'for', 'the', 'iPhone', 'was', 'beautiful', 'and', 'worked', 'great', '.', 'The', 'screen', 'protector', 'that', 'was', 'additional', 'came', 'cracked', '(', 'Not', 'a', 'big', 'deal', 'as', 'it', 'was', 'extra', 'in', 'my', 'eyes', ')', '.', 'I', 'did', 'need', 'to', 'use', 'the', 'X-Acto', 'knife', 'to', 'cut', 'off', 'part', 'of', 'a', 'plastic', 'piece', 'to', 'make', 'the', 'final', 'fit', '.', 'So', 'yes', 'I', 'modified', 'the', 'screen', 'and', 'instructions', 'but', 'ended', 'up', 'working', 'great', 'for', 'me.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'I', 'was', 'very', 'careful', 'with', 'all', 'of', 'the', 'circuit', 'boards', 'and', 'connections', 'as', 'recommended', 'on', 'the', 'YouTube', 'video', '.', 'My', 'screen', 'replacement', 'was', 'very', 'successful', 'and', 'I', '’', 'm', 'very', 'happy', 'with', 'how', 'it', 'turned', 'out', '.']}, {'title': 'Decent', 'text': 'Lasted about 9 months then the lock button broke off. Decent product but costing scrapes off like crazy.  I shredded this case. Protected my phone tho', 'tokens': ['Lasted', 'about', '9', 'months', 'then', 'the', 'lock', 'button', 'broke', 'off', '.', 'Decent', 'product', 'but', 'costing', 'scrapes', 'off', 'like', 'crazy', '.', 'I', 'shredded', 'this', 'case', '.', 'Protected', 'my', 'phone', 'tho']}, {'title': 'LOVE IT!', 'text': 'LOVE THIS CASE! Works better than my expensive $35 cases! lol', 'tokens': ['LOVE', 'THIS', 'CASE', '!', 'Works', 'better', 'than', 'my', 'expensive', '$', '35', 'cases', '!', 'lol']}]\n"
     ]
    }
   ],
   "source": [
    "# Découpage en mots des documents\n",
    "def tokenize_document(document):\n",
    "    return word_tokenize(document['text'])\n",
    "\n",
    "# Tokenization des documents\n",
    "for document in documents:\n",
    "    document['tokens'] = tokenize_document(document)\n",
    "    \n",
    "# Les 5 premiers documents tokenizés\n",
    "print(documents[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'No white background! It’s clear!', 'text': 'I bought this bc I thought it had the nice white background. Turns out it’s clear & since my phone is blue it doesn’t look anything like this.  If I had known that I would have purchased something else. It works ok.', 'tokens': ['I', 'bought', 'this', 'bc', 'I', 'thought', 'it', 'had', 'the', 'nice', 'white', 'background', '.', 'Turns', 'out', 'it', '’', 's', 'clear', '&', 'since', 'my', 'phone', 'is', 'blue', 'it', 'doesn', '’', 't', 'look', 'anything', 'like', 'this', '.', 'If', 'I', 'had', 'known', 'that', 'I', 'would', 'have', 'purchased', 'something', 'else', '.', 'It', 'works', 'ok', '.'], 'lemmas': ['I', 'bought', 'this', 'bc', 'I', 'thought', 'it', 'had', 'the', 'nice', 'white', 'background', '.', 'Turns', 'out', 'it', '’', 's', 'clear', '&', 'since', 'my', 'phone', 'is', 'blue', 'it', 'doesn', '’', 't', 'look', 'anything', 'like', 'this', '.', 'If', 'I', 'had', 'known', 'that', 'I', 'would', 'have', 'purchased', 'something', 'else', '.', 'It', 'work', 'ok', '.']}, {'title': 'Awesome!  Great price!  Works well!', 'text': 'Perfect. How pissed am I that I recently paid $20 for 1 Fitbit cable and promptly lost the damned thing?  Extremely pissed!  I keep the spare in my medicine bag so hopefully I won’t lose it and my grandson can’t get to it and try to use it as a belt or a dog leash or any of the other nutty things he’s been using the other one for.', 'tokens': ['Perfect', '.', 'How', 'pissed', 'am', 'I', 'that', 'I', 'recently', 'paid', '$', '20', 'for', '1', 'Fitbit', 'cable', 'and', 'promptly', 'lost', 'the', 'damned', 'thing', '?', 'Extremely', 'pissed', '!', 'I', 'keep', 'the', 'spare', 'in', 'my', 'medicine', 'bag', 'so', 'hopefully', 'I', 'won', '’', 't', 'lose', 'it', 'and', 'my', 'grandson', 'can', '’', 't', 'get', 'to', 'it', 'and', 'try', 'to', 'use', 'it', 'as', 'a', 'belt', 'or', 'a', 'dog', 'leash', 'or', 'any', 'of', 'the', 'other', 'nutty', 'things', 'he', '’', 's', 'been', 'using', 'the', 'other', 'one', 'for', '.'], 'lemmas': ['Perfect', '.', 'How', 'pissed', 'am', 'I', 'that', 'I', 'recently', 'paid', '$', '20', 'for', '1', 'Fitbit', 'cable', 'and', 'promptly', 'lost', 'the', 'damned', 'thing', '?', 'Extremely', 'pissed', '!', 'I', 'keep', 'the', 'spare', 'in', 'my', 'medicine', 'bag', 'so', 'hopefully', 'I', 'won', '’', 't', 'lose', 'it', 'and', 'my', 'grandson', 'can', '’', 't', 'get', 'to', 'it', 'and', 'try', 'to', 'use', 'it', 'a', 'a', 'belt', 'or', 'a', 'dog', 'leash', 'or', 'any', 'of', 'the', 'other', 'nutty', 'thing', 'he', '’', 's', 'been', 'using', 'the', 'other', 'one', 'for', '.']}, {'title': 'Worked but took an hour to install', 'text': 'Overall very happy with the end result. If you hate puzzles don’t do it. I love puzzles and it worked for me. Took a lot of concentration and attention to detail and about an hour! The YouTube video helped a ton with installing the new screen. Highly recommend using a how to video when replacing your screen. The tools and supplies they provided were adequate. I did use additional tools from my home to successfully installed a new screen. My screws on the inside of the iPhone were stuck and I had to use an X-Acto knife to get them to come out. The glass Screen for the iPhone was beautiful and worked great. The screen protector that was additional came cracked (Not a big deal as it was extra in my eyes). I did need to use the X-Acto knife to cut off part of a plastic piece to make the final fit. So yes I modified the screen and instructions but ended up working great for me.<br /><br />I was very careful with all of the circuit boards and connections as recommended on the YouTube video. My screen replacement was very successful and I’m very happy with how it turned out.', 'tokens': ['Overall', 'very', 'happy', 'with', 'the', 'end', 'result', '.', 'If', 'you', 'hate', 'puzzles', 'don', '’', 't', 'do', 'it', '.', 'I', 'love', 'puzzles', 'and', 'it', 'worked', 'for', 'me', '.', 'Took', 'a', 'lot', 'of', 'concentration', 'and', 'attention', 'to', 'detail', 'and', 'about', 'an', 'hour', '!', 'The', 'YouTube', 'video', 'helped', 'a', 'ton', 'with', 'installing', 'the', 'new', 'screen', '.', 'Highly', 'recommend', 'using', 'a', 'how', 'to', 'video', 'when', 'replacing', 'your', 'screen', '.', 'The', 'tools', 'and', 'supplies', 'they', 'provided', 'were', 'adequate', '.', 'I', 'did', 'use', 'additional', 'tools', 'from', 'my', 'home', 'to', 'successfully', 'installed', 'a', 'new', 'screen', '.', 'My', 'screws', 'on', 'the', 'inside', 'of', 'the', 'iPhone', 'were', 'stuck', 'and', 'I', 'had', 'to', 'use', 'an', 'X-Acto', 'knife', 'to', 'get', 'them', 'to', 'come', 'out', '.', 'The', 'glass', 'Screen', 'for', 'the', 'iPhone', 'was', 'beautiful', 'and', 'worked', 'great', '.', 'The', 'screen', 'protector', 'that', 'was', 'additional', 'came', 'cracked', '(', 'Not', 'a', 'big', 'deal', 'as', 'it', 'was', 'extra', 'in', 'my', 'eyes', ')', '.', 'I', 'did', 'need', 'to', 'use', 'the', 'X-Acto', 'knife', 'to', 'cut', 'off', 'part', 'of', 'a', 'plastic', 'piece', 'to', 'make', 'the', 'final', 'fit', '.', 'So', 'yes', 'I', 'modified', 'the', 'screen', 'and', 'instructions', 'but', 'ended', 'up', 'working', 'great', 'for', 'me.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'I', 'was', 'very', 'careful', 'with', 'all', 'of', 'the', 'circuit', 'boards', 'and', 'connections', 'as', 'recommended', 'on', 'the', 'YouTube', 'video', '.', 'My', 'screen', 'replacement', 'was', 'very', 'successful', 'and', 'I', '’', 'm', 'very', 'happy', 'with', 'how', 'it', 'turned', 'out', '.'], 'lemmas': ['Overall', 'very', 'happy', 'with', 'the', 'end', 'result', '.', 'If', 'you', 'hate', 'puzzle', 'don', '’', 't', 'do', 'it', '.', 'I', 'love', 'puzzle', 'and', 'it', 'worked', 'for', 'me', '.', 'Took', 'a', 'lot', 'of', 'concentration', 'and', 'attention', 'to', 'detail', 'and', 'about', 'an', 'hour', '!', 'The', 'YouTube', 'video', 'helped', 'a', 'ton', 'with', 'installing', 'the', 'new', 'screen', '.', 'Highly', 'recommend', 'using', 'a', 'how', 'to', 'video', 'when', 'replacing', 'your', 'screen', '.', 'The', 'tool', 'and', 'supply', 'they', 'provided', 'were', 'adequate', '.', 'I', 'did', 'use', 'additional', 'tool', 'from', 'my', 'home', 'to', 'successfully', 'installed', 'a', 'new', 'screen', '.', 'My', 'screw', 'on', 'the', 'inside', 'of', 'the', 'iPhone', 'were', 'stuck', 'and', 'I', 'had', 'to', 'use', 'an', 'X-Acto', 'knife', 'to', 'get', 'them', 'to', 'come', 'out', '.', 'The', 'glass', 'Screen', 'for', 'the', 'iPhone', 'wa', 'beautiful', 'and', 'worked', 'great', '.', 'The', 'screen', 'protector', 'that', 'wa', 'additional', 'came', 'cracked', '(', 'Not', 'a', 'big', 'deal', 'a', 'it', 'wa', 'extra', 'in', 'my', 'eye', ')', '.', 'I', 'did', 'need', 'to', 'use', 'the', 'X-Acto', 'knife', 'to', 'cut', 'off', 'part', 'of', 'a', 'plastic', 'piece', 'to', 'make', 'the', 'final', 'fit', '.', 'So', 'yes', 'I', 'modified', 'the', 'screen', 'and', 'instruction', 'but', 'ended', 'up', 'working', 'great', 'for', 'me.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'I', 'wa', 'very', 'careful', 'with', 'all', 'of', 'the', 'circuit', 'board', 'and', 'connection', 'a', 'recommended', 'on', 'the', 'YouTube', 'video', '.', 'My', 'screen', 'replacement', 'wa', 'very', 'successful', 'and', 'I', '’', 'm', 'very', 'happy', 'with', 'how', 'it', 'turned', 'out', '.']}, {'title': 'Decent', 'text': 'Lasted about 9 months then the lock button broke off. Decent product but costing scrapes off like crazy.  I shredded this case. Protected my phone tho', 'tokens': ['Lasted', 'about', '9', 'months', 'then', 'the', 'lock', 'button', 'broke', 'off', '.', 'Decent', 'product', 'but', 'costing', 'scrapes', 'off', 'like', 'crazy', '.', 'I', 'shredded', 'this', 'case', '.', 'Protected', 'my', 'phone', 'tho'], 'lemmas': ['Lasted', 'about', '9', 'month', 'then', 'the', 'lock', 'button', 'broke', 'off', '.', 'Decent', 'product', 'but', 'costing', 'scrape', 'off', 'like', 'crazy', '.', 'I', 'shredded', 'this', 'case', '.', 'Protected', 'my', 'phone', 'tho']}, {'title': 'LOVE IT!', 'text': 'LOVE THIS CASE! Works better than my expensive $35 cases! lol', 'tokens': ['LOVE', 'THIS', 'CASE', '!', 'Works', 'better', 'than', 'my', 'expensive', '$', '35', 'cases', '!', 'lol'], 'lemmas': ['LOVE', 'THIS', 'CASE', '!', 'Works', 'better', 'than', 'my', 'expensive', '$', '35', 'case', '!', 'lol']}]\n"
     ]
    }
   ],
   "source": [
    "# Initialisation du lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatization des tokens\n",
    "for document in documents:\n",
    "    document['lemmas'] = [lemmatizer.lemmatize(token) for token in document['tokens']]\n",
    "    \n",
    "# Les 5 premiers documents lemmatisés\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: No white background! It’s clear!\n",
      "Cleaned Tokens: ['bought', 'bc', 'thought', 'nice', 'white', 'background', 'turn', 'clear', 'since', 'phone', 'blue', 'look', 'anything', 'like', 'known', 'would', 'purchased', 'something', 'else', 'work', 'ok']\n",
      "--------------------------------------------------\n",
      "Title: Awesome!  Great price!  Works well!\n",
      "Cleaned Tokens: ['perfect', 'pissed', 'recently', 'paid', 'fitbit', 'cable', 'promptly', 'lost', 'damned', 'thing', 'extremely', 'pissed', 'keep', 'spare', 'medicine', 'bag', 'hopefully', 'lose', 'grandson', 'get', 'try', 'use', 'belt', 'dog', 'leash', 'nutty', 'thing', 'using', 'one']\n",
      "--------------------------------------------------\n",
      "Title: Worked but took an hour to install\n",
      "Cleaned Tokens: ['overall', 'happy', 'end', 'result', 'hate', 'puzzle', 'love', 'puzzle', 'worked', 'took', 'lot', 'concentration', 'attention', 'detail', 'hour', 'youtube', 'video', 'helped', 'ton', 'installing', 'new', 'screen', 'highly', 'recommend', 'using', 'video', 'replacing', 'screen', 'tool', 'supply', 'provided', 'adequate', 'use', 'additional', 'tool', 'home', 'successfully', 'installed', 'new', 'screen', 'screw', 'inside', 'iphone', 'stuck', 'use', 'knife', 'get', 'come', 'glass', 'screen', 'iphone', 'beautiful', 'worked', 'great', 'screen', 'protector', 'additional', 'came', 'cracked', 'big', 'deal', 'extra', 'eye', 'need', 'use', 'knife', 'cut', 'part', 'plastic', 'piece', 'make', 'final', 'fit', 'yes', 'modified', 'screen', 'instruction', 'ended', 'working', 'great', 'br', 'br', 'careful', 'circuit', 'board', 'connection', 'recommended', 'youtube', 'video', 'screen', 'replacement', 'successful', 'happy', 'turned']\n",
      "--------------------------------------------------\n",
      "Title: Decent\n",
      "Cleaned Tokens: ['lasted', 'month', 'lock', 'button', 'broke', 'decent', 'product', 'costing', 'scrape', 'like', 'crazy', 'shredded', 'case', 'protected', 'phone', 'tho']\n",
      "--------------------------------------------------\n",
      "Title: LOVE IT!\n",
      "Cleaned Tokens: ['love', 'case', 'work', 'better', 'expensive', 'case', 'lol']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Charger les stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Charger les données\n",
    "with open(reviews_file, 'r') as file:\n",
    "    reviews = [json.loads(line) for line in file]\n",
    "\n",
    "# Sélection des champs pertinents (title et text) et création d'une liste de documents\n",
    "documents = [{'title': review['title'], 'text': review['text']} for review in reviews]\n",
    "\n",
    "# Découpage en mots des documents\n",
    "def tokenize_document(document):\n",
    "    return word_tokenize(document['text'].lower())  # Mettre en minuscule et découper en tokens\n",
    "\n",
    "# Initialisation du lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Nettoyage des documents\n",
    "def clean_document(document):\n",
    "    tokens = tokenize_document(document)\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(token)  # Lemmatization\n",
    "        for token in tokens\n",
    "        if token.isalpha() and token not in stop_words  # Exclusion des chiffres et stopwords\n",
    "    ]\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Nettoyage des documents\n",
    "for document in documents:\n",
    "    document['cleaned_tokens'] = clean_document(document)\n",
    "\n",
    "# Afficher les 5 premiers documents nettoyés\n",
    "for doc in documents[:5]:\n",
    "    print(f\"Title: {doc['title']}\")\n",
    "    print(f\"Cleaned Tokens: {doc['cleaned_tokens']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les stopwords (par exemple : \"the\", \"and\", \"is\", etc.) sont des mots fréquents qui n'apportent pas d'information significative sur le contenu ou le contexte du texte. En les excluant, on se concentre sur les mots clés qui sont plus représentatifs du sujet ou du thème du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De même, les caractères non alphabétiques (nombres, ponctuations, URL, etc.) ne contribuent pas à la compréhension du contenu sémantique. Leur présence peut biaiser les analyses ou introduire des informations inutiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les avis filtrés ont été sauvegardés dans filtered_reviews.json.\n"
     ]
    }
   ],
   "source": [
    "# Charger les stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Fichier d'entrée et de sortie\n",
    "input_file = 'reviews.jsonl'\n",
    "output_file = 'filtered_reviews.json'\n",
    "\n",
    "# Charger les données depuis le fichier JSONL\n",
    "with open(input_file, 'r') as file:\n",
    "    reviews = [json.loads(line) for line in file]\n",
    "\n",
    "# Initialisation du lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Fonction pour nettoyer un document\n",
    "def clean_document(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenisation et mise en minuscule\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(token)  # Lemmatization\n",
    "        for token in tokens\n",
    "        if token.isalpha() and token not in stop_words  # Garder uniquement les mots pertinents\n",
    "    ]\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Traitement des avis\n",
    "filtered_reviews = []\n",
    "for review in reviews:\n",
    "    filtered_review = {\n",
    "        'title': review.get('title', ''),\n",
    "        'filtered_tokens': clean_document(review.get('text', ''))\n",
    "    }\n",
    "    filtered_reviews.append(filtered_review)\n",
    "\n",
    "# Sauvegarde dans un fichier JSON\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(filtered_reviews, file, indent=4)\n",
    "\n",
    "print(f\"Les avis filtrés ont été sauvegardés dans {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2. Custering non supervisé des documents pour identifier des topics et mots-clés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Génération des embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 32/32 [00:02<00:00, 15.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06282184  0.05036959  0.08300743 ...  0.04501068 -0.02777872\n",
      "   0.08845506]\n",
      " [-0.05745867  0.06035928  0.02571245 ... -0.09513349 -0.07105947\n",
      "   0.03159803]\n",
      " [-0.04276645  0.03064304  0.12047602 ...  0.07421792  0.00493353\n",
      "   0.04009885]\n",
      " [-0.12291301  0.03837333  0.10437208 ... -0.06036926  0.00318576\n",
      "   0.05050042]\n",
      " [-0.03441106  0.02525463  0.02802392 ... -0.07721563 -0.01886018\n",
      "   0.12872452]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle pré-entraîné\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extraire les textes des documents\n",
    "texts = [doc['text'] for doc in documents]\n",
    "\n",
    "# Générer les embeddings pour les textes\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Afficher les embeddings des 5 premiers documents\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j'ai choisi d'utiliser dbscan car pas besoin de définir le nombre de clusters, ce qui est pratique pour des données non structurées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les résultats de DBSCAN ont été sauvegardés dans dbscan_clustered_reviews.json.\n"
     ]
    }
   ],
   "source": [
    "# Paramètres\n",
    "eps = 0.5  # Distance maximale pour regrouper des points\n",
    "min_samples = 2  # Nombre minimum de points pour former un cluster\n",
    "\n",
    "# Charger les données filtrées\n",
    "input_file = 'filtered_reviews.json'\n",
    "with open(input_file, 'r') as file:\n",
    "    filtered_reviews = json.load(file)\n",
    "\n",
    "# Préparation des données textuelles\n",
    "documents = [\" \".join(review['filtered_tokens']) for review in filtered_reviews]\n",
    "\n",
    "# TF-IDF vectorisation\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# DBSCAN Clustering\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')\n",
    "dbscan_labels = dbscan.fit_predict(X)\n",
    "\n",
    "# Ajouter les labels DBSCAN aux documents\n",
    "for i, review in enumerate(filtered_reviews):\n",
    "    review['dbscan_label'] = int(dbscan_labels[i])\n",
    "\n",
    "# Sauvegarder les résultats dans un fichier JSON\n",
    "output_file = 'dbscan_clustered_reviews.json'\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(filtered_reviews, file, indent=4)\n",
    "\n",
    "print(f\"Les résultats de DBSCAN ont été sauvegardés dans {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les documents regroupés par cluster ont été sauvegardés dans grouped_by_cluster.json.\n"
     ]
    }
   ],
   "source": [
    "# Charger les résultats DBSCAN\n",
    "input_file = 'dbscan_clustered_reviews.json'\n",
    "with open(input_file, 'r') as file:\n",
    "    clustered_reviews = json.load(file)\n",
    "\n",
    "# Regrouper les documents par cluster\n",
    "clusters = defaultdict(list)\n",
    "for review in clustered_reviews:\n",
    "    cluster_label = review['dbscan_label']\n",
    "    clusters[cluster_label].append(review)\n",
    "\n",
    "# Convertir en dict normal pour la sauvegarde JSON\n",
    "clusters = dict(clusters)\n",
    "\n",
    "# Sauvegarder les résultats\n",
    "output_file = 'grouped_by_cluster.json'\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(clusters, file, indent=4)\n",
    "\n",
    "print(f\"Les documents regroupés par cluster ont été sauvegardés dans {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 10 mots les plus fréquents par cluster ont été sauvegardés dans top_words_per_cluster.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    }
   ],
   "source": [
    "# Assurez-vous que le tokenizer et les stop words sont disponibles\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Charger les résultats DBSCAN\n",
    "input_file = 'dbscan_clustered_reviews.json'\n",
    "with open(input_file, 'r') as file:\n",
    "    clustered_reviews = json.load(file)\n",
    "\n",
    "# Charger les stopwords en anglais\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Fonction pour nettoyer et filtrer les tokens (enlever stopwords et ponctuation)\n",
    "def clean_tokens(tokens):\n",
    "    return [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "# Regrouper les documents par cluster\n",
    "clusters = defaultdict(list)\n",
    "for review in clustered_reviews:\n",
    "    cluster_label = review['dbscan_label']\n",
    "    clusters[cluster_label].append(review)\n",
    "\n",
    "# Calcul des fréquences des mots pour chaque cluster\n",
    "cluster_word_frequencies = {}\n",
    "\n",
    "for cluster_label, documents in clusters.items():\n",
    "    all_tokens = []\n",
    "    \n",
    "    for document in documents:\n",
    "        # Utilisation de 'filtered_tokens' qui contient déjà les tokens filtrés\n",
    "        if 'filtered_tokens' in document:\n",
    "            tokens = document['filtered_tokens']  # Utilisation des tokens filtrés\n",
    "            cleaned_tokens = clean_tokens(tokens)  # Nettoyage des tokens (enlever stop words et ponctuation)\n",
    "            all_tokens.extend(cleaned_tokens)\n",
    "        else:\n",
    "            print(f\"Clé 'filtered_tokens' non trouvée dans le document avec le label {cluster_label}.\")\n",
    "    \n",
    "    # Compter les fréquences des mots pour ce cluster\n",
    "    word_counts = Counter(all_tokens)\n",
    "    cluster_word_frequencies[cluster_label] = word_counts\n",
    "\n",
    "# Identification des 10 mots les plus fréquents pour chaque cluster\n",
    "top_words_per_cluster = {}\n",
    "\n",
    "for cluster_label, word_counts in cluster_word_frequencies.items():\n",
    "    top_words = word_counts.most_common(10)  # Récupérer les 10 mots les plus fréquents\n",
    "    top_words_per_cluster[cluster_label] = top_words\n",
    "\n",
    "# Sauvegarder les résultats dans un fichier JSON\n",
    "output_file = 'top_words_per_cluster.json'\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(top_words_per_cluster, file, indent=4)\n",
    "\n",
    "print(f\"Les 10 mots les plus fréquents par cluster ont été sauvegardés dans {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 3. Analyse des sentiments des avis clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.0, 5.0, 5.0, 4.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "with open(reviews_file, 'r') as file:\n",
    "    reviews = [json.loads(line) for line in file]\n",
    "\n",
    "# Extraire les textes et les notes\n",
    "texts = [review['text'] for review in reviews]\n",
    "ratings = [review['rating'] for review in reviews]\n",
    "\n",
    "# Afficher les 5 premières notes\n",
    "print(ratings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Charger le modèle et le tokenizer\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "# Définir un Dataset personnalisé\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "# Charger les données dans un DataLoader\n",
    "batch_size = 16  # Ajustez selon vos ressources\n",
    "dataset = ReviewsDataset(texts)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les prédictions\n",
    "predicted_ratings = []\n",
    "\n",
    "# Désactiver le calcul des gradients pour économiser de la mémoire\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        # Tokenisation\n",
    "        tokens = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        # Passer les données dans le modèle\n",
    "        outputs = model(**tokens)\n",
    "        \n",
    "        # Appliquer Softmax\n",
    "        probabilities = softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Prédire la classe (1 à 5)\n",
    "        predictions = torch.argmax(probabilities, dim=-1) + 1  # Ajouter 1 car les classes commencent à 0\n",
    "        predicted_ratings.extend(predictions.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrélation de Pearson: 0.8167668695013106\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Associer les sentiments prédits à des scores numériques\n",
    "predicted_scores = predicted_ratings\n",
    "\n",
    "# Calculer la corrélation de Pearson entre les notes réelles et les prédictions\n",
    "correlation, _ = pearsonr(ratings, predicted_scores)\n",
    "\n",
    "print(f\"Corrélation de Pearson: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce projet a été une expérience enrichissante mais complexe pour moi. Étant ma première fois à travailler sur des tâches de traitement du langage naturel (NLP). J'ai rencontré plusieurs défis comme le traitement de données textuelles, la gestion des embeddings, et l'application d'algorithmes de clustering comme DBSCAN étaient de nouveaux concepts pour moi, et cela a demandé une adaptation.\n",
    "\n",
    "Un des principaux obstacles que j'ai rencontrés durant ce projet était un problème de kernel sur mon ordinateur. Cela m'a obligé à passer beaucoup de temps à déboguer et à résoudre ce problème avant de pouvoir continuer à travailler sur le projet. Ce contretemps a ralenti mes progrès, mais m'a aussi permis de renforcer ma compréhension des environnements de travail et des outils de développement.\n",
    "\n",
    "Le projet m'a permis de mieux comprendre les étapes clés du prétraitement des données, du nettoyage des avis clients, et de l'exploration de la similarité sémantique entre les documents. Cependant, la mise en œuvre des techniques de clustering et l'interprétation des résultats ont parfois été difficiles, en particulier en ce qui concerne le choix des bons paramètres et l'évaluation de la qualité des clusters.\n",
    "\n",
    "En dépit des difficultés techniques et des défis rencontrés, ce projet m'a donné une première introduction précieuse au domaine du NLP, et m'a permis de mieux saisir comment ces technologies peuvent être utilisées pour extraire des insights à partir de données textuelles. J'espère que, avec plus de pratique, je serai plus à l'aise avec ces concepts et que je pourrai aborder de manière plus fluide des projets similaires à l'avenir.\n",
    "\n",
    "En résumé, bien que ce projet ait été complexe et stimulant pour moi, il m'a offert une opportunité d'apprendre et de progresser dans un domaine que je trouve à la fois fascinant et prometteur."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
